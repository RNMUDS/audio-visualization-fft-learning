{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. 音声処理\n",
    "\n",
    "人の音声の分析と処理技術を学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基本周波数（ピッチ）追跡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceProcessor:\n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "    def create_voice_like_signal(self, duration=3.0):\n",
    "        \"\"\"音声に似た信号を生成\"\"\"\n",
    "        t = np.linspace(0, duration, int(self.sample_rate * duration), False)\n",
    "        \n",
    "        # 基本周波数の変化（男性の話し声：80-200Hz）\n",
    "        f0_base = 120  # Hz\n",
    "        f0_variation = 30 * np.sin(2 * np.pi * 0.5 * t)  # 抑揚\n",
    "        f0 = f0_base + f0_variation\n",
    "        \n",
    "        # 累積位相\n",
    "        phase = np.cumsum(2 * np.pi * f0 / self.sample_rate)\n",
    "        \n",
    "        # 声帯振動（のこぎり波ベース）\n",
    "        glottal_pulse = signal.sawtooth(phase)\n",
    "        \n",
    "        # フォルマント（口の形による共鳴）\n",
    "        # フォルマント1: 700Hz, フォルマント2: 1220Hz, フォルマント3: 2600Hz\n",
    "        formants = [700, 1220, 2600]\n",
    "        formant_gains = [1.0, 0.7, 0.3]\n",
    "        \n",
    "        voice_signal = np.zeros_like(t)\n",
    "        \n",
    "        for formant_freq, gain in zip(formants, formant_gains):\n",
    "            # バンドパスフィルタでフォルマントを作成\n",
    "            nyquist = self.sample_rate / 2\n",
    "            low = (formant_freq - 100) / nyquist\n",
    "            high = (formant_freq + 100) / nyquist\n",
    "            \n",
    "            if high < 1.0:\n",
    "                b, a = signal.butter(2, [low, high], btype='band')\n",
    "                formant_signal = signal.filtfilt(b, a, glottal_pulse)\n",
    "                voice_signal += gain * formant_signal\n",
    "        \n",
    "        # 振幅エンベロープ（発話の区切り）\n",
    "        envelope = np.ones_like(t)\n",
    "        \n",
    "        # 無音区間を追加\n",
    "        silence_1 = (t > 0.8) & (t < 1.0)\n",
    "        silence_2 = (t > 2.0) & (t < 2.3)\n",
    "        envelope[silence_1] = 0.1\n",
    "        envelope[silence_2] = 0.1\n",
    "        \n",
    "        voice_signal *= envelope\n",
    "        \n",
    "        return t, voice_signal, f0\n",
    "        \n",
    "    def pitch_tracking_autocorr(self, audio, frame_size=1024, hop_size=256):\n",
    "        \"\"\"自己相関によるピッチ追跡\"\"\"\n",
    "        pitches = []\n",
    "        confidences = []\n",
    "        times = []\n",
    "        \n",
    "        for i in range(0, len(audio) - frame_size, hop_size):\n",
    "            frame = audio[i:i + frame_size]\n",
    "            time = i / self.sample_rate\n",
    "            times.append(time)\n",
    "            \n",
    "            # 自己相関\n",
    "            autocorr = np.correlate(frame, frame, mode='full')\n",
    "            autocorr = autocorr[len(autocorr)//2:]\n",
    "            \n",
    "            # 正規化\n",
    "            if autocorr[0] > 0:\n",
    "                autocorr = autocorr / autocorr[0]\n",
    "            \n",
    "            # ピッチ範囲（80-400Hz）\n",
    "            min_period = int(self.sample_rate / 400)\n",
    "            max_period = int(self.sample_rate / 80)\n",
    "            \n",
    "            if max_period < len(autocorr):\n",
    "                search_range = autocorr[min_period:max_period]\n",
    "                \n",
    "                if len(search_range) > 0:\n",
    "                    max_idx = np.argmax(search_range)\n",
    "                    period = max_idx + min_period\n",
    "                    pitch = self.sample_rate / period\n",
    "                    confidence = search_range[max_idx]\n",
    "                else:\n",
    "                    pitch = 0\n",
    "                    confidence = 0\n",
    "            else:\n",
    "                pitch = 0\n",
    "                confidence = 0\n",
    "                \n",
    "            pitches.append(pitch)\n",
    "            confidences.append(confidence)\n",
    "        \n",
    "        return np.array(times), np.array(pitches), np.array(confidences)\n",
    "        \n",
    "    def formant_analysis(self, audio, frame_size=1024):\n",
    "        \"\"\"フォルマント分析（簡易版）\"\"\"\n",
    "        # LPC（線形予測符号化）の簡易実装\n",
    "        windowed = audio * np.hanning(len(audio))\n",
    "        \n",
    "        # FFTによるスペクトル\n",
    "        spectrum = np.abs(fft(windowed))\n",
    "        freqs = fftfreq(len(windowed), 1/self.sample_rate)\n",
    "        \n",
    "        # 正の周波数のみ\n",
    "        positive_mask = freqs >= 0\n",
    "        freqs_pos = freqs[positive_mask]\n",
    "        spectrum_pos = spectrum[positive_mask]\n",
    "        \n",
    "        # ピーク検出でフォルマント推定\n",
    "        peaks, _ = signal.find_peaks(spectrum_pos, \n",
    "                                   height=np.max(spectrum_pos) * 0.1,\n",
    "                                   distance=int(200 / (self.sample_rate / len(spectrum_pos))))\n",
    "        \n",
    "        formant_freqs = freqs_pos[peaks]\n",
    "        formant_amps = spectrum_pos[peaks]\n",
    "        \n",
    "        # 低い順にソート\n",
    "        sorted_indices = np.argsort(formant_freqs)\n",
    "        formant_freqs = formant_freqs[sorted_indices]\n",
    "        formant_amps = formant_amps[sorted_indices]\n",
    "        \n",
    "        return formant_freqs[:3], formant_amps[:3]  # 最初の3つのフォルマント\n",
    "\n",
    "# 音声処理のデモ\n",
    "voice_proc = VoiceProcessor()\n",
    "\n",
    "# 音声様信号の生成\n",
    "t_voice, voice_signal, true_f0 = voice_proc.create_voice_like_signal()\n",
    "\n",
    "# ピッチ追跡\n",
    "pitch_times, detected_pitches, pitch_confidences = voice_proc.pitch_tracking_autocorr(voice_signal)\n",
    "\n",
    "# フォルマント分析（音声の中間部分）\n",
    "mid_start = len(voice_signal) // 3\n",
    "mid_end = 2 * len(voice_signal) // 3\n",
    "formant_freqs, formant_amps = voice_proc.formant_analysis(voice_signal[mid_start:mid_end])\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 音声波形\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(t_voice, voice_signal, 'b-', linewidth=1)\n",
    "plt.title('合成音声信号')\n",
    "plt.ylabel('振幅')\n",
    "plt.grid(True)\n",
    "\n",
    "# スペクトログラム\n",
    "plt.subplot(3, 2, 2)\n",
    "f, t_spec, Sxx = signal.spectrogram(voice_signal, voice_proc.sample_rate, \n",
    "                                   nperseg=512, noverlap=256)\n",
    "plt.pcolormesh(t_spec, f, 10 * np.log10(Sxx + 1e-10), \n",
    "              shading='gouraud', cmap='viridis')\n",
    "plt.title('音声スペクトログラム')\n",
    "plt.ylabel('周波数 (Hz)')\n",
    "plt.ylim(0, 3000)\n",
    "plt.colorbar(label='振幅 (dB)')\n",
    "\n",
    "# フォルマントをマーク\n",
    "for formant_freq in formant_freqs:\n",
    "    plt.axhline(y=formant_freq, color='red', linestyle='--', alpha=0.8, linewidth=2)\n",
    "\n",
    "# ピッチ追跡結果\n",
    "plt.subplot(3, 2, 3)\n",
    "# 信頼度によるフィルタリング\n",
    "confident_mask = pitch_confidences > 0.3\n",
    "plt.plot(pitch_times, detected_pitches, 'ro-', markersize=4, alpha=0.7, label='検出ピッチ')\n",
    "plt.plot(pitch_times[confident_mask], detected_pitches[confident_mask], \n",
    "         'go-', markersize=6, label='高信頼度')\n",
    "\n",
    "# 真のピッチをプロット\n",
    "t_interp = np.interp(pitch_times, t_voice, true_f0)\n",
    "plt.plot(pitch_times, t_interp, 'b-', linewidth=2, label='真のピッチ')\n",
    "\n",
    "plt.title('ピッチ追跡結果')\n",
    "plt.xlabel('時間 (秒)')\n",
    "plt.ylabel('基本周波数 (Hz)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(50, 250)\n",
    "\n",
    "# ピッチ信頼度\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(pitch_times, pitch_confidences, 'g-', linewidth=2)\n",
    "plt.axhline(y=0.3, color='r', linestyle='--', label='信頼度しきい値')\n",
    "plt.title('ピッチ検出信頼度')\n",
    "plt.xlabel('時間 (秒)')\n",
    "plt.ylabel('信頼度')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# フォルマント分析結果\n",
    "plt.subplot(3, 2, 5)\n",
    "mid_spectrum = np.abs(fft(voice_signal[mid_start:mid_end] * \n",
    "                         np.hanning(mid_end - mid_start)))\n",
    "mid_freqs = fftfreq(mid_end - mid_start, 1/voice_proc.sample_rate)\n",
    "positive_mask = mid_freqs >= 0\n",
    "\n",
    "plt.plot(mid_freqs[positive_mask], 20 * np.log10(mid_spectrum[positive_mask] + 1e-10), \n",
    "         'b-', linewidth=1)\n",
    "\n",
    "# 検出されたフォルマントをマーク\n",
    "for i, (freq, amp) in enumerate(zip(formant_freqs, formant_amps)):\n",
    "    plt.axvline(x=freq, color='red', linestyle='--', alpha=0.8)\n",
    "    plt.text(freq, 20 * np.log10(amp + 1e-10) + 5, f'F{i+1}\\n{freq:.0f}Hz', \n",
    "            ha='center', fontsize=10, color='red')\n",
    "\n",
    "plt.title('フォルマント分析')\n",
    "plt.xlabel('周波数 (Hz)')\n",
    "plt.ylabel('振幅 (dB)')\n",
    "plt.grid(True)\n",
    "plt.xlim(0, 3500)\n",
    "\n",
    "# 統計情報\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.axis('off')\n",
    "\n",
    "# ピッチ統計\n",
    "valid_pitches = detected_pitches[confident_mask]\n",
    "if len(valid_pitches) > 0:\n",
    "    pitch_mean = np.mean(valid_pitches)\n",
    "    pitch_std = np.std(valid_pitches)\n",
    "    pitch_range = np.max(valid_pitches) - np.min(valid_pitches)\n",
    "else:\n",
    "    pitch_mean = pitch_std = pitch_range = 0\n",
    "\n",
    "stats_text = f\"\"\"\n",
    "音声分析結果\n",
    "\n",
    "ピッチ統計:\n",
    "  平均: {pitch_mean:.1f} Hz\n",
    "  標準偏差: {pitch_std:.1f} Hz\n",
    "  範囲: {pitch_range:.1f} Hz\n",
    "  \n",
    "フォルマント:\n",
    "\"\"\"\n",
    "\n",
    "for i, freq in enumerate(formant_freqs):\n",
    "    stats_text += f\"  F{i+1}: {freq:.0f} Hz\\n\"\n",
    "\n",
    "stats_text += f\"\"\"\n",
    "品質指標:\n",
    "  ピッチ検出率: {np.mean(confident_mask)*100:.1f}%\n",
    "  平均信頼度: {np.mean(pitch_confidences):.2f}\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.05, 0.95, stats_text, fontsize=12, verticalalignment='top',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"音声分析の要素:\")\n",
    "print(\"1. 基本周波数（F0）: 声の高さ\")\n",
    "print(\"2. フォルマント: 母音の特徴\")\n",
    "print(\"3. ピッチ追跡: 抑揚の分析\")\n",
    "print(\"4. 信頼度評価: 検出精度の指標\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 音声強調と雑音除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceEnhancement:\n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "    def spectral_subtraction(self, noisy_signal, noise_segment, alpha=2.0):\n",
    "        \"\"\"スペクトル減算による雑音除去\"\"\"\n",
    "        # ノイズのスペクトル推定\n",
    "        noise_spectrum = np.abs(fft(noise_segment * np.hanning(len(noise_segment))))\n",
    "        noise_power = noise_spectrum ** 2\n",
    "        \n",
    "        # フレームごとの処理\n",
    "        frame_size = len(noise_segment)\n",
    "        hop_size = frame_size // 2\n",
    "        enhanced_signal = np.zeros_like(noisy_signal)\n",
    "        \n",
    "        for i in range(0, len(noisy_signal) - frame_size, hop_size):\n",
    "            frame = noisy_signal[i:i + frame_size]\n",
    "            windowed_frame = frame * np.hanning(frame_size)\n",
    "            \n",
    "            # FFT\n",
    "            frame_fft = fft(windowed_frame)\n",
    "            frame_magnitude = np.abs(frame_fft)\n",
    "            frame_phase = np.angle(frame_fft)\n",
    "            frame_power = frame_magnitude ** 2\n",
    "            \n",
    "            # スペクトル減算\n",
    "            enhanced_power = frame_power - alpha * noise_power\n",
    "            \n",
    "            # 負の値を制限（オーバーサブトラクション防止）\n",
    "            enhanced_power = np.maximum(enhanced_power, 0.1 * frame_power)\n",
    "            \n",
    "            # 振幅の復元\n",
    "            enhanced_magnitude = np.sqrt(enhanced_power)\n",
    "            \n",
    "            # 逆FFT\n",
    "            enhanced_fft = enhanced_magnitude * np.exp(1j * frame_phase)\n",
    "            enhanced_frame = np.real(np.fft.ifft(enhanced_fft))\n",
    "            \n",
    "            # オーバーラップアッド\n",
    "            end_idx = min(i + frame_size, len(enhanced_signal))\n",
    "            enhanced_signal[i:end_idx] += enhanced_frame[:end_idx-i] * np.hanning(frame_size)[:end_idx-i]\n",
    "        \n",
    "        return enhanced_signal\n",
    "        \n",
    "    def wiener_filter(self, noisy_signal, noise_segment):\n",
    "        \"\"\"ウィーナーフィルタによる雑音除去\"\"\"\n",
    "        # ノイズパワー推定\n",
    "        noise_spectrum = np.abs(fft(noise_segment * np.hanning(len(noise_segment))))\n",
    "        noise_power = np.mean(noise_spectrum ** 2)\n",
    "        \n",
    "        # 信号全体のFFT\n",
    "        signal_fft = fft(noisy_signal * np.hanning(len(noisy_signal)))\n",
    "        signal_power = np.abs(signal_fft) ** 2\n",
    "        \n",
    "        # ウィーナーフィルタの係数\n",
    "        wiener_gain = signal_power / (signal_power + noise_power)\n",
    "        \n",
    "        # フィルタリング\n",
    "        enhanced_fft = signal_fft * wiener_gain\n",
    "        enhanced_signal = np.real(np.fft.ifft(enhanced_fft))\n",
    "        \n",
    "        return enhanced_signal\n",
    "        \n",
    "    def preemphasis(self, signal, alpha=0.97):\n",
    "        \"\"\"プリエンファシス（高周波強調）\"\"\"\n",
    "        return np.append(signal[0], signal[1:] - alpha * signal[:-1])\n",
    "        \n",
    "    def deemphasis(self, signal, alpha=0.97):\n",
    "        \"\"\"ディエンファシス（プリエンファシスの逆）\"\"\"\n",
    "        deemphasized = np.zeros_like(signal)\n",
    "        deemphasized[0] = signal[0]\n",
    "        for i in range(1, len(signal)):\n",
    "            deemphasized[i] = signal[i] + alpha * deemphasized[i-1]\n",
    "        return deemphasized\n",
    "        \n",
    "    def voice_activity_detection(self, signal, frame_size=512, threshold=0.02):\n",
    "        \"\"\"音声区間検出（VAD）\"\"\"\n",
    "        hop_size = frame_size // 2\n",
    "        vad_flags = []\n",
    "        times = []\n",
    "        \n",
    "        for i in range(0, len(signal) - frame_size, hop_size):\n",
    "            frame = signal[i:i + frame_size]\n",
    "            time = i / self.sample_rate\n",
    "            times.append(time)\n",
    "            \n",
    "            # エネルギーベースのVAD\n",
    "            energy = np.sum(frame ** 2) / frame_size\n",
    "            \n",
    "            # ゼロクロッシング率\n",
    "            zcr = np.sum(np.diff(np.sign(frame)) != 0) / (frame_size - 1)\n",
    "            \n",
    "            # 判定（エネルギーとZCRの組み合わせ）\n",
    "            is_voice = (energy > threshold) and (zcr < 0.3)\n",
    "            vad_flags.append(is_voice)\n",
    "            \n",
    "        return np.array(times), np.array(vad_flags)\n",
    "\n",
    "# 音声強調のデモ\n",
    "enhancer = VoiceEnhancement()\n",
    "\n",
    "# クリーンな音声信号を生成\n",
    "clean_signal = voice_signal.copy()\n",
    "\n",
    "# 様々なノイズを追加\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. ホワイトノイズ\n",
    "white_noise = np.random.normal(0, 0.1, len(clean_signal))\n",
    "noisy_white = clean_signal + white_noise\n",
    "\n",
    "# 2. 60Hzハム\n",
    "hum_noise = 0.05 * np.sin(2 * np.pi * 60 * t_voice)\n",
    "noisy_hum = clean_signal + hum_noise\n",
    "\n",
    "# 3. 複合ノイズ\n",
    "pink_noise = np.random.normal(0, 0.08, len(clean_signal))\n",
    "# 簡易ピンクノイズフィルタ\n",
    "b_pink, a_pink = signal.butter(1, 0.1, btype='low')\n",
    "pink_noise = signal.filtfilt(b_pink, a_pink, pink_noise)\n",
    "complex_noise = white_noise * 0.5 + hum_noise + pink_noise\n",
    "noisy_complex = clean_signal + complex_noise\n",
    "\n",
    "# ノイズセグメント（最初の0.3秒をノイズのみと仮定）\n",
    "noise_segment_length = int(0.3 * enhancer.sample_rate)\n",
    "noise_segment = complex_noise[:noise_segment_length]\n",
    "\n",
    "# 雑音除去処理\n",
    "enhanced_spectral = enhancer.spectral_subtraction(noisy_complex, noise_segment, alpha=2.0)\n",
    "enhanced_wiener = enhancer.wiener_filter(noisy_complex, noise_segment)\n",
    "\n",
    "# プリエンファシス処理\n",
    "preemphasized = enhancer.preemphasis(clean_signal)\n",
    "deemphasized = enhancer.deemphasis(preemphasized)\n",
    "\n",
    "# VAD\n",
    "vad_times, vad_flags = enhancer.voice_activity_detection(clean_signal)\n",
    "\n",
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "# 元信号とノイズ付き信号\n",
    "plt.subplot(4, 3, 1)\n",
    "plt.plot(t_voice, clean_signal, 'b-', linewidth=1, label='クリーン')\n",
    "plt.plot(t_voice, noisy_complex, 'r-', linewidth=1, alpha=0.7, label='ノイズ付き')\n",
    "plt.title('元信号 vs ノイズ付き信号')\n",
    "plt.ylabel('振幅')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# スペクトル比較\n",
    "plt.subplot(4, 3, 2)\n",
    "clean_spectrum = np.abs(fft(clean_signal * np.hanning(len(clean_signal))))\n",
    "noisy_spectrum = np.abs(fft(noisy_complex * np.hanning(len(noisy_complex))))\n",
    "freqs = fftfreq(len(clean_signal), 1/enhancer.sample_rate)\n",
    "positive_mask = freqs >= 0\n",
    "\n",
    "plt.plot(freqs[positive_mask], 20 * np.log10(clean_spectrum[positive_mask] + 1e-10), \n",
    "         'b-', label='クリーン', linewidth=2)\n",
    "plt.plot(freqs[positive_mask], 20 * np.log10(noisy_spectrum[positive_mask] + 1e-10), \n",
    "         'r-', alpha=0.7, label='ノイズ付き', linewidth=2)\n",
    "plt.title('スペクトル比較')\n",
    "plt.xlabel('周波数 (Hz)')\n",
    "plt.ylabel('振幅 (dB)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlim(0, 4000)\n",
    "\n",
    "# スペクトル減算結果\n",
    "plt.subplot(4, 3, 3)\n",
    "plt.plot(t_voice, clean_signal, 'b-', linewidth=1, label='クリーン')\n",
    "plt.plot(t_voice[:len(enhanced_spectral)], enhanced_spectral, 'g-', \n",
    "         linewidth=1, label='スペクトル減算')\n",
    "plt.title('スペクトル減算による雑音除去')\n",
    "plt.ylabel('振幅')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ウィーナーフィルタ結果\n",
    "plt.subplot(4, 3, 4)\n",
    "plt.plot(t_voice, clean_signal, 'b-', linewidth=1, label='クリーン')\n",
    "plt.plot(t_voice[:len(enhanced_wiener)], enhanced_wiener, 'm-', \n",
    "         linewidth=1, label='ウィーナーフィルタ')\n",
    "plt.title('ウィーナーフィルタによる雑音除去')\n",
    "plt.ylabel('振幅')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 強調処理のスペクトログラム比較\n",
    "plt.subplot(4, 3, 5)\n",
    "f_noisy, t_noisy, Sxx_noisy = signal.spectrogram(noisy_complex, enhancer.sample_rate, \n",
    "                                                 nperseg=256, noverlap=128)\n",
    "plt.pcolormesh(t_noisy, f_noisy, 10 * np.log10(Sxx_noisy + 1e-10), \n",
    "              shading='gouraud', cmap='Reds')\n",
    "plt.title('ノイズ付き（スペクトログラム）')\n",
    "plt.ylabel('周波数 (Hz)')\n",
    "plt.ylim(0, 3000)\n",
    "plt.colorbar(label='振幅 (dB)')\n",
    "\n",
    "plt.subplot(4, 3, 6)\n",
    "# スペクトル減算後のスペクトログラム\n",
    "if len(enhanced_spectral) > 256:\n",
    "    f_enh, t_enh, Sxx_enh = signal.spectrogram(enhanced_spectral, enhancer.sample_rate, \n",
    "                                              nperseg=256, noverlap=128)\n",
    "    plt.pcolormesh(t_enh, f_enh, 10 * np.log10(Sxx_enh + 1e-10), \n",
    "                  shading='gouraud', cmap='Greens')\n",
    "plt.title('スペクトル減算後（スペクトログラム）')\n",
    "plt.ylabel('周波数 (Hz)')\n",
    "plt.ylim(0, 3000)\n",
    "plt.colorbar(label='振幅 (dB)')\n",
    "\n",
    "# プリエンファシス効果\n",
    "plt.subplot(4, 3, 7)\n",
    "plt.plot(t_voice, clean_signal, 'b-', linewidth=1, label='元信号')\n",
    "plt.plot(t_voice[:len(preemphasized)], preemphasized, 'orange', \n",
    "         linewidth=1, label='プリエンファシス後')\n",
    "plt.title('プリエンファシス処理')\n",
    "plt.ylabel('振幅')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# VAD結果\n",
    "plt.subplot(4, 3, 8)\n",
    "plt.plot(t_voice, clean_signal, 'b-', linewidth=1, alpha=0.7, label='音声信号')\n",
    "\n",
    "# VADフラグを音声信号に重ねて表示\n",
    "for i, (time, is_voice) in enumerate(zip(vad_times, vad_flags)):\n",
    "    color = 'green' if is_voice else 'red'\n",
    "    alpha = 0.3\n",
    "    \n",
    "    # 時間区間をハイライト\n",
    "    if i < len(vad_times) - 1:\n",
    "        next_time = vad_times[i + 1]\n",
    "        plt.axvspan(time, next_time, color=color, alpha=alpha)\n",
    "\n",
    "plt.title('音声区間検出（VAD）\\n緑：音声, 赤：無音')\n",
    "plt.xlabel('時間 (秒)')\n",
    "plt.ylabel('振幅')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 処理結果比較（SNR計算）\n",
    "plt.subplot(4, 3, 9)\n",
    "\n",
    "def calculate_snr(clean, noisy):\n",
    "    signal_power = np.mean(clean ** 2)\n",
    "    noise_power = np.mean((noisy - clean) ** 2)\n",
    "    if noise_power > 0:\n",
    "        return 10 * np.log10(signal_power / noise_power)\n",
    "    else:\n",
    "        return float('inf')\n",
    "\n",
    "# SNR計算\n",
    "snr_noisy = calculate_snr(clean_signal, noisy_complex)\n",
    "snr_spectral = calculate_snr(clean_signal, enhanced_spectral[:len(clean_signal)])\n",
    "snr_wiener = calculate_snr(clean_signal, enhanced_wiener[:len(clean_signal)])\n",
    "\n",
    "methods = ['ノイズ付き', 'スペクトル減算', 'ウィーナーフィルタ']\n",
    "snr_values = [snr_noisy, snr_spectral, snr_wiener]\n",
    "colors = ['red', 'green', 'magenta']\n",
    "\n",
    "bars = plt.bar(methods, snr_values, color=colors, alpha=0.7)\n",
    "plt.title('SNR改善効果')\n",
    "plt.ylabel('SNR (dB)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for bar, snr in zip(bars, snr_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "            f'{snr:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "# 統計情報\n",
    "plt.subplot(4, 3, (10, 12))\n",
    "plt.axis('off')\n",
    "\n",
    "stats_text = f\"\"\"\n",
    "音声強調処理結果\n",
    "\n",
    "SNR改善:\n",
    "  元のSNR: {snr_noisy:.1f} dB\n",
    "  スペクトル減算後: {snr_spectral:.1f} dB\n",
    "  ウィーナーフィルタ後: {snr_wiener:.1f} dB\n",
    "  \n",
    "改善量:\n",
    "  スペクトル減算: {snr_spectral - snr_noisy:.1f} dB\n",
    "  ウィーナーフィルタ: {snr_wiener - snr_noisy:.1f} dB\n",
    "\n",
    "VAD結果:\n",
    "  音声率: {np.mean(vad_flags)*100:.1f}%\n",
    "  無音率: {(1-np.mean(vad_flags))*100:.1f}%\n",
    "\n",
    "処理の特徴:\n",
    "  • スペクトル減算: 定常ノイズに効果的\n",
    "  • ウィーナーフィルタ: より自然な音質\n",
    "  • プリエンファシス: 高周波強調\n",
    "  • VAD: 音声区間の自動検出\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.05, 0.95, stats_text, fontsize=11, verticalalignment='top',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"音声強調技術:\")\n",
    "print(\"1. スペクトル減算: ノイズスペクトルを推定して除去\")\n",
    "print(\"2. ウィーナーフィルタ: 最適化された雑音除去\")\n",
    "print(\"3. プリエンファシス: 高周波成分の強調\")\n",
    "print(\"4. VAD: 音声区間の自動検出\")\n",
    "print(\"5. SNR: 信号対雑音比による性能評価\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習問題\n",
    "\n",
    "1. より高精度なピッチ追跡アルゴリズム（YIN、CREPE等）を実装してみましょう\n",
    "2. 適応的な雑音除去フィルタを作成してみましょう\n",
    "3. 音声のフォルマント追跡による母音認識システムを構築してみましょう"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}