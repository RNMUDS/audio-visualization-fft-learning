{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# リアルタイム音響処理とモニタリング\n",
    "\n",
    "このノートブックでは、リアルタイムでの音響処理とモニタリングのシミュレーションを学習します。\n",
    "実際のマイク入力の代わりに、生成した音響データを使用してリアルタイム処理の概念を学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from collections import deque\n",
    "import time\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import threading\n",
    "import queue\n",
    "from pydub import AudioSegment\n",
    "from pydub.generators import Sine, Square, Sawtooth\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 日本語フォントの設定\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# Jupyter notebook内でアニメーションを表示\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. リアルタイムデータストリームのシミュレーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioStreamSimulator:\n",
    "    \"\"\"\n",
    "    音響ストリームをシミュレートするクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_rate=1000, chunk_size=256):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.chunk_size = chunk_size\n",
    "        self.time_counter = 0\n",
    "        self.is_running = False\n",
    "        self.data_queue = queue.Queue()\n",
    "        \n",
    "    def generate_audio_chunk(self):\n",
    "        \"\"\"\n",
    "        音響チャンクを生成\n",
    "        \"\"\"\n",
    "        t_start = self.time_counter / self.sample_rate\n",
    "        t_end = (self.time_counter + self.chunk_size) / self.sample_rate\n",
    "        t = np.linspace(t_start, t_end, self.chunk_size, endpoint=False)\n",
    "        \n",
    "        # 複数の音響成分を組み合わせ\n",
    "        # 基本周波数（時間とともに変化）\n",
    "        freq1 = 200 + 100 * np.sin(2 * np.pi * 0.1 * t_start)\n",
    "        signal1 = np.sin(2 * np.pi * freq1 * t)\n",
    "        \n",
    "        # 高調波\n",
    "        freq2 = 300 + 50 * np.cos(2 * np.pi * 0.05 * t_start)\n",
    "        signal2 = 0.5 * np.sin(2 * np.pi * freq2 * t)\n",
    "        \n",
    "        # ノイズ\n",
    "        noise = 0.1 * np.random.randn(self.chunk_size)\n",
    "        \n",
    "        # 不定期なパルス\n",
    "        if np.random.random() < 0.1:  # 10%の確率でパルス\n",
    "            pulse_start = np.random.randint(0, self.chunk_size - 50)\n",
    "            pulse = np.zeros(self.chunk_size)\n",
    "            pulse[pulse_start:pulse_start+50] = 2 * np.sin(2 * np.pi * 500 * \n",
    "                                                          t[pulse_start:pulse_start+50])\n",
    "        else:\n",
    "            pulse = np.zeros(self.chunk_size)\n",
    "        \n",
    "        # 全ての成分を合成\n",
    "        combined_signal = signal1 + signal2 + noise + pulse\n",
    "        \n",
    "        self.time_counter += self.chunk_size\n",
    "        return combined_signal, t\n",
    "    \n",
    "    def start_stream(self):\n",
    "        \"\"\"\n",
    "        ストリーミング開始\n",
    "        \"\"\"\n",
    "        self.is_running = True\n",
    "        \n",
    "        def stream_thread():\n",
    "            while self.is_running:\n",
    "                chunk, time_chunk = self.generate_audio_chunk()\n",
    "                self.data_queue.put((chunk, time_chunk))\n",
    "                time.sleep(self.chunk_size / self.sample_rate)  # リアルタイムシミュレーション\n",
    "        \n",
    "        self.thread = threading.Thread(target=stream_thread)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop_stream(self):\n",
    "        \"\"\"\n",
    "        ストリーミング停止\n",
    "        \"\"\"\n",
    "        self.is_running = False\n",
    "        if hasattr(self, 'thread'):\n",
    "            self.thread.join()\n",
    "    \n",
    "    def get_chunk(self):\n",
    "        \"\"\"\n",
    "        次のチャンクを取得\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.data_queue.get_nowait()\n",
    "        except queue.Empty:\n",
    "            return None, None\n",
    "\n",
    "# シミュレータのテスト\n",
    "simulator = AudioStreamSimulator()\n",
    "chunk, t = simulator.generate_audio_chunk()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(t, chunk)\n",
    "plt.title('Sample Audio Chunk')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Chunk size: {len(chunk)} samples\")\n",
    "print(f\"Duration: {len(chunk)/simulator.sample_rate:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. リアルタイム波形モニタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeWaveformMonitor:\n",
    "    \"\"\"\n",
    "    リアルタイム波形モニタ\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size=2000, sample_rate=1000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.sample_rate = sample_rate\n",
    "        self.audio_buffer = deque(maxlen=buffer_size)\n",
    "        self.time_buffer = deque(maxlen=buffer_size)\n",
    "        \n",
    "        # 初期化\n",
    "        for _ in range(buffer_size):\n",
    "            self.audio_buffer.append(0)\n",
    "            self.time_buffer.append(0)\n",
    "    \n",
    "    def update_buffer(self, new_data, new_time):\n",
    "        \"\"\"\n",
    "        バッファを新しいデータで更新\n",
    "        \"\"\"\n",
    "        for sample, t in zip(new_data, new_time):\n",
    "            self.audio_buffer.append(sample)\n",
    "            self.time_buffer.append(t)\n",
    "    \n",
    "    def get_display_data(self):\n",
    "        \"\"\"\n",
    "        表示用データを取得\n",
    "        \"\"\"\n",
    "        return np.array(self.time_buffer), np.array(self.audio_buffer)\n",
    "\n",
    "# モニタの作成とテスト\n",
    "monitor = RealTimeWaveformMonitor()\n",
    "\n",
    "# 複数チャンクでテスト\n",
    "simulator = AudioStreamSimulator()\n",
    "for i in range(10):\n",
    "    chunk, t_chunk = simulator.generate_audio_chunk()\n",
    "    monitor.update_buffer(chunk, t_chunk)\n",
    "\n",
    "time_data, audio_data = monitor.get_display_data()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(time_data, audio_data)\n",
    "plt.title('Real-time Waveform Buffer')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Buffer contains {len(audio_data)} samples\")\n",
    "print(f\"Time range: {time_data[0]:.3f} - {time_data[-1]:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. リアルタイムスペクトラム解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeSpectrumAnalyzer:\n",
    "    \"\"\"\n",
    "    リアルタイムスペクトラム解析器\n",
    "    \"\"\"\n",
    "    def __init__(self, fft_size=512, sample_rate=1000, overlap=0.5):\n",
    "        self.fft_size = fft_size\n",
    "        self.sample_rate = sample_rate\n",
    "        self.overlap = overlap\n",
    "        self.hop_size = int(fft_size * (1 - overlap))\n",
    "        \n",
    "        # 窓関数\n",
    "        self.window = signal.windows.hann(fft_size)\n",
    "        \n",
    "        # 周波数軸\n",
    "        self.freqs = fftfreq(fft_size, 1/sample_rate)[:fft_size//2]\n",
    "        \n",
    "        # スペクトログラム用バッファ\n",
    "        self.spectrogram_buffer = deque(maxlen=100)  # 100フレーム分\n",
    "        \n",
    "        # 音響特徴量の履歴\n",
    "        self.spectral_centroid_history = deque(maxlen=200)\n",
    "        self.spectral_rolloff_history = deque(maxlen=200)\n",
    "        self.rms_history = deque(maxlen=200)\n",
    "        \n",
    "    def analyze_chunk(self, audio_chunk):\n",
    "        \"\"\"\n",
    "        音響チャンクを解析\n",
    "        \"\"\"\n",
    "        if len(audio_chunk) < self.fft_size:\n",
    "            # ゼロパディング\n",
    "            padded_chunk = np.zeros(self.fft_size)\n",
    "            padded_chunk[:len(audio_chunk)] = audio_chunk\n",
    "            audio_chunk = padded_chunk\n",
    "        \n",
    "        # 窓関数の適用\n",
    "        windowed_signal = audio_chunk[:self.fft_size] * self.window\n",
    "        \n",
    "        # FFT\n",
    "        fft_result = fft(windowed_signal)\n",
    "        magnitude_spectrum = np.abs(fft_result[:self.fft_size//2])\n",
    "        power_spectrum = magnitude_spectrum ** 2\n",
    "        \n",
    "        # スペクトログラムバッファに追加\n",
    "        self.spectrogram_buffer.append(magnitude_spectrum)\n",
    "        \n",
    "        # 音響特徴量の計算\n",
    "        # スペクトラル重心\n",
    "        if np.sum(power_spectrum) > 0:\n",
    "            spectral_centroid = np.sum(self.freqs * power_spectrum) / np.sum(power_spectrum)\n",
    "        else:\n",
    "            spectral_centroid = 0\n",
    "        \n",
    "        # スペクトラルロールオフ（85%エネルギーポイント）\n",
    "        cumsum_power = np.cumsum(power_spectrum)\n",
    "        if cumsum_power[-1] > 0:\n",
    "            rolloff_idx = np.where(cumsum_power >= 0.85 * cumsum_power[-1])[0]\n",
    "            spectral_rolloff = self.freqs[rolloff_idx[0]] if len(rolloff_idx) > 0 else 0\n",
    "        else:\n",
    "            spectral_rolloff = 0\n",
    "        \n",
    "        # RMS\n",
    "        rms = np.sqrt(np.mean(audio_chunk ** 2))\n",
    "        \n",
    "        # 履歴に追加\n",
    "        self.spectral_centroid_history.append(spectral_centroid)\n",
    "        self.spectral_rolloff_history.append(spectral_rolloff)\n",
    "        self.rms_history.append(rms)\n",
    "        \n",
    "        return {\n",
    "            'magnitude_spectrum': magnitude_spectrum,\n",
    "            'power_spectrum': power_spectrum,\n",
    "            'spectral_centroid': spectral_centroid,\n",
    "            'spectral_rolloff': spectral_rolloff,\n",
    "            'rms': rms\n",
    "        }\n",
    "    \n",
    "    def get_spectrogram(self):\n",
    "        \"\"\"\n",
    "        スペクトログラムデータを取得\n",
    "        \"\"\"\n",
    "        if len(self.spectrogram_buffer) == 0:\n",
    "            return np.zeros((len(self.freqs), 1))\n",
    "        return np.array(list(self.spectrogram_buffer)).T\n",
    "    \n",
    "    def get_feature_history(self):\n",
    "        \"\"\"\n",
    "        特徴量履歴を取得\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'spectral_centroid': np.array(self.spectral_centroid_history),\n",
    "            'spectral_rolloff': np.array(self.spectral_rolloff_history),\n",
    "            'rms': np.array(self.rms_history)\n",
    "        }\n",
    "\n",
    "# スペクトラム解析器のテスト\n",
    "analyzer = RealTimeSpectrumAnalyzer()\n",
    "simulator = AudioStreamSimulator()\n",
    "\n",
    "# 複数チャンクの解析\n",
    "results = []\n",
    "for i in range(20):\n",
    "    chunk, _ = simulator.generate_audio_chunk()\n",
    "    result = analyzer.analyze_chunk(chunk)\n",
    "    results.append(result)\n",
    "\n",
    "# 結果の可視化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 最新のスペクトラム\n",
    "axes[0, 0].semilogy(analyzer.freqs, results[-1]['magnitude_spectrum'])\n",
    "axes[0, 0].set_title('Latest Magnitude Spectrum')\n",
    "axes[0, 0].set_xlabel('Frequency (Hz)')\n",
    "axes[0, 0].set_ylabel('Magnitude')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# スペクトログラム\n",
    "spectrogram = analyzer.get_spectrogram()\n",
    "im = axes[0, 1].imshow(spectrogram, aspect='auto', origin='lower', \n",
    "                       extent=[0, len(results), 0, analyzer.freqs[-1]], cmap='hot')\n",
    "axes[0, 1].set_title('Spectrogram')\n",
    "axes[0, 1].set_xlabel('Time Frame')\n",
    "axes[0, 1].set_ylabel('Frequency (Hz)')\n",
    "plt.colorbar(im, ax=axes[0, 1], label='Magnitude')\n",
    "\n",
    "# 特徴量の時間変化\n",
    "features = analyzer.get_feature_history()\n",
    "time_frames = np.arange(len(features['spectral_centroid']))\n",
    "\n",
    "axes[1, 0].plot(time_frames, features['spectral_centroid'], label='Spectral Centroid')\n",
    "axes[1, 0].plot(time_frames, features['spectral_rolloff'], label='Spectral Rolloff')\n",
    "axes[1, 0].set_title('Spectral Features Over Time')\n",
    "axes[1, 0].set_xlabel('Time Frame')\n",
    "axes[1, 0].set_ylabel('Frequency (Hz)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# RMS履歴\n",
    "axes[1, 1].plot(time_frames, features['rms'])\n",
    "axes[1, 1].set_title('RMS Level Over Time')\n",
    "axes[1, 1].set_xlabel('Time Frame')\n",
    "axes[1, 1].set_ylabel('RMS')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Analyzed {len(results)} audio chunks\")\n",
    "print(f\"Latest spectral centroid: {results[-1]['spectral_centroid']:.2f} Hz\")\n",
    "print(f\"Latest RMS level: {results[-1]['rms']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. リアルタイム音響イベント検出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEventDetector:\n",
    "    \"\"\"\n",
    "    音響イベント検出器\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_rate=1000):\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # 閾値設定\n",
    "        self.onset_threshold = 0.3  # オンセット検出閾値\n",
    "        self.silence_threshold = 0.01  # 無音検出閾値\n",
    "        self.frequency_peak_threshold = 0.1  # 周波数ピーク検出閾値\n",
    "        \n",
    "        # 履歴バッファ\n",
    "        self.rms_history = deque(maxlen=10)\n",
    "        self.spectral_flux_history = deque(maxlen=10)\n",
    "        self.previous_spectrum = None\n",
    "        \n",
    "        # イベント履歴\n",
    "        self.events = []\n",
    "        self.frame_count = 0\n",
    "        \n",
    "    def detect_events(self, audio_chunk, spectrum_result):\n",
    "        \"\"\"\n",
    "        音響イベントを検出\n",
    "        \"\"\"\n",
    "        events_detected = []\n",
    "        current_time = self.frame_count * len(audio_chunk) / self.sample_rate\n",
    "        \n",
    "        # RMS計算\n",
    "        rms = np.sqrt(np.mean(audio_chunk ** 2))\n",
    "        self.rms_history.append(rms)\n",
    "        \n",
    "        # 1. オンセット検出（RMSの急激な増加）\n",
    "        if len(self.rms_history) >= 3:\n",
    "            rms_diff = rms - np.mean(list(self.rms_history)[:-1])\n",
    "            if rms_diff > self.onset_threshold:\n",
    "                events_detected.append({\n",
    "                    'type': 'onset',\n",
    "                    'time': current_time,\n",
    "                    'value': rms_diff,\n",
    "                    'description': f'Audio onset detected (RMS increase: {rms_diff:.3f})'\n",
    "                })\n",
    "        \n",
    "        # 2. 無音検出\n",
    "        if rms < self.silence_threshold:\n",
    "            events_detected.append({\n",
    "                'type': 'silence',\n",
    "                'time': current_time,\n",
    "                'value': rms,\n",
    "                'description': f'Silence detected (RMS: {rms:.4f})'\n",
    "            })\n",
    "        \n",
    "        # 3. スペクトラルフラックス（周波数内容の変化）\n",
    "        current_spectrum = spectrum_result['magnitude_spectrum']\n",
    "        if self.previous_spectrum is not None:\n",
    "            spectral_flux = np.sum(np.maximum(0, current_spectrum - self.previous_spectrum))\n",
    "            self.spectral_flux_history.append(spectral_flux)\n",
    "            \n",
    "            # 高いスペクトラルフラックスは新しい音の出現を示す\n",
    "            if len(self.spectral_flux_history) >= 3:\n",
    "                flux_threshold = np.mean(list(self.spectral_flux_history)[:-1]) + 2 * np.std(list(self.spectral_flux_history)[:-1])\n",
    "                if spectral_flux > flux_threshold and spectral_flux > 10:\n",
    "                    events_detected.append({\n",
    "                        'type': 'spectral_change',\n",
    "                        'time': current_time,\n",
    "                        'value': spectral_flux,\n",
    "                        'description': f'Spectral change detected (flux: {spectral_flux:.2f})'\n",
    "                    })\n",
    "        \n",
    "        # 4. 周波数ピーク検出\n",
    "        spectrum = spectrum_result['magnitude_spectrum']\n",
    "        peaks, properties = signal.find_peaks(spectrum, height=np.max(spectrum) * self.frequency_peak_threshold)\n",
    "        \n",
    "        if len(peaks) > 3:  # 多くのピークがある場合\n",
    "            # 周波数を計算\n",
    "            freqs = np.fft.fftfreq(len(spectrum) * 2, 1/self.sample_rate)[:len(spectrum)]\n",
    "            peak_freqs = freqs[peaks]\n",
    "            \n",
    "            events_detected.append({\n",
    "                'type': 'harmonic_content',\n",
    "                'time': current_time,\n",
    "                'value': len(peaks),\n",
    "                'description': f'Harmonic content detected ({len(peaks)} peaks at {peak_freqs[:3]:.0f} Hz)'\n",
    "            })\n",
    "        \n",
    "        # 5. 高エネルギー検出\n",
    "        if rms > 1.0:  # 高いRMSレベル\n",
    "            events_detected.append({\n",
    "                'type': 'high_energy',\n",
    "                'time': current_time,\n",
    "                'value': rms,\n",
    "                'description': f'High energy event (RMS: {rms:.3f})'\n",
    "            })\n",
    "        \n",
    "        # イベントを履歴に追加\n",
    "        self.events.extend(events_detected)\n",
    "        \n",
    "        # 前のスペクトラムを保存\n",
    "        self.previous_spectrum = current_spectrum.copy()\n",
    "        self.frame_count += 1\n",
    "        \n",
    "        return events_detected\n",
    "    \n",
    "    def get_event_summary(self):\n",
    "        \"\"\"\n",
    "        イベントサマリーを取得\n",
    "        \"\"\"\n",
    "        if not self.events:\n",
    "            return \"No events detected\"\n",
    "        \n",
    "        event_types = {}\n",
    "        for event in self.events:\n",
    "            event_type = event['type']\n",
    "            if event_type not in event_types:\n",
    "                event_types[event_type] = 0\n",
    "            event_types[event_type] += 1\n",
    "        \n",
    "        return event_types\n",
    "\n",
    "# イベント検出器のテスト\n",
    "detector = AudioEventDetector()\n",
    "analyzer = RealTimeSpectrumAnalyzer()\n",
    "simulator = AudioStreamSimulator()\n",
    "\n",
    "print(\"Processing audio stream and detecting events...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_events = []\n",
    "for i in range(30):\n",
    "    chunk, _ = simulator.generate_audio_chunk()\n",
    "    spectrum_result = analyzer.analyze_chunk(chunk)\n",
    "    events = detector.detect_events(chunk, spectrum_result)\n",
    "    \n",
    "    # イベントが検出された場合に表示\n",
    "    for event in events:\n",
    "        print(f\"[{event['time']:.2f}s] {event['type'].upper()}: {event['description']}\")\n",
    "    \n",
    "    all_events.extend(events)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Event Summary:\")\n",
    "summary = detector.get_event_summary()\n",
    "for event_type, count in summary.items():\n",
    "    print(f\"  {event_type}: {count} occurrences\")\n",
    "\n",
    "print(f\"\\nTotal events detected: {len(all_events)}\")\n",
    "print(f\"Analysis duration: {detector.frame_count * 256 / 1000:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. リアルタイム音響効果の適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeAudioProcessor:\n",
    "    \"\"\"\n",
    "    リアルタイム音響処理器\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_rate=1000):\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # エフェクトパラメータ\n",
    "        self.effects = {\n",
    "            'gain': 1.0,\n",
    "            'lowpass_freq': 500,\n",
    "            'highpass_freq': 50,\n",
    "            'echo_delay': 0.5,\n",
    "            'echo_decay': 0.3,\n",
    "            'compressor_threshold': 0.7,\n",
    "            'compressor_ratio': 4.0\n",
    "        }\n",
    "        \n",
    "        # フィルタ係数\n",
    "        self.update_filters()\n",
    "        \n",
    "        # エコー用ディレイバッファ\n",
    "        delay_samples = int(self.effects['echo_delay'] * sample_rate)\n",
    "        self.delay_buffer = deque(maxlen=delay_samples, iterable=[0] * delay_samples)\n",
    "        \n",
    "        # コンプレッサ用状態\n",
    "        self.compressor_state = 0\n",
    "        \n",
    "    def update_filters(self):\n",
    "        \"\"\"\n",
    "        フィルタ係数を更新\n",
    "        \"\"\"\n",
    "        # ローパスフィルタ\n",
    "        nyquist = self.sample_rate / 2\n",
    "        low_freq = min(self.effects['lowpass_freq'], nyquist - 1)\n",
    "        self.lowpass_b, self.lowpass_a = signal.butter(4, low_freq / nyquist, btype='low')\n",
    "        \n",
    "        # ハイパスフィルタ\n",
    "        high_freq = max(self.effects['highpass_freq'], 1)\n",
    "        self.highpass_b, self.highpass_a = signal.butter(4, high_freq / nyquist, btype='high')\n",
    "    \n",
    "    def apply_gain(self, audio_chunk):\n",
    "        \"\"\"\n",
    "        ゲインを適用\n",
    "        \"\"\"\n",
    "        return audio_chunk * self.effects['gain']\n",
    "    \n",
    "    def apply_filters(self, audio_chunk):\n",
    "        \"\"\"\n",
    "        フィルタを適用\n",
    "        \"\"\"\n",
    "        # ローパス\n",
    "        filtered = signal.lfilter(self.lowpass_b, self.lowpass_a, audio_chunk)\n",
    "        # ハイパス\n",
    "        filtered = signal.lfilter(self.highpass_b, self.highpass_a, filtered)\n",
    "        return filtered\n",
    "    \n",
    "    def apply_echo(self, audio_chunk):\n",
    "        \"\"\"\n",
    "        エコーエフェクトを適用\n",
    "        \"\"\"\n",
    "        output = np.zeros_like(audio_chunk)\n",
    "        \n",
    "        for i, sample in enumerate(audio_chunk):\n",
    "            # ディレイバッファから古いサンプルを取得\n",
    "            delayed_sample = self.delay_buffer[0]\n",
    "            \n",
    "            # エコーを加算\n",
    "            output[i] = sample + delayed_sample * self.effects['echo_decay']\n",
    "            \n",
    "            # 新しいサンプルをディレイバッファに追加\n",
    "            self.delay_buffer.append(sample)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def apply_compressor(self, audio_chunk):\n",
    "        \"\"\"\n",
    "        シンプルなコンプレッサを適用\n",
    "        \"\"\"\n",
    "        threshold = self.effects['compressor_threshold']\n",
    "        ratio = self.effects['compressor_ratio']\n",
    "        \n",
    "        output = np.zeros_like(audio_chunk)\n",
    "        \n",
    "        for i, sample in enumerate(audio_chunk):\n",
    "            abs_sample = abs(sample)\n",
    "            \n",
    "            if abs_sample > threshold:\n",
    "                # 圧縮を適用\n",
    "                excess = abs_sample - threshold\n",
    "                compressed_excess = excess / ratio\n",
    "                compressed_amplitude = threshold + compressed_excess\n",
    "                output[i] = np.sign(sample) * compressed_amplitude\n",
    "            else:\n",
    "                output[i] = sample\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def process_chunk(self, audio_chunk, effects_enabled=None):\n",
    "        \"\"\"\n",
    "        音響チャンクを処理\n",
    "        \"\"\"\n",
    "        if effects_enabled is None:\n",
    "            effects_enabled = ['gain', 'filters', 'echo', 'compressor']\n",
    "        \n",
    "        processed = audio_chunk.copy()\n",
    "        \n",
    "        # エフェクトチェーンの適用\n",
    "        if 'gain' in effects_enabled:\n",
    "            processed = self.apply_gain(processed)\n",
    "        \n",
    "        if 'filters' in effects_enabled:\n",
    "            processed = self.apply_filters(processed)\n",
    "        \n",
    "        if 'echo' in effects_enabled:\n",
    "            processed = self.apply_echo(processed)\n",
    "        \n",
    "        if 'compressor' in effects_enabled:\n",
    "            processed = self.apply_compressor(processed)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def set_effect_parameter(self, effect_name, value):\n",
    "        \"\"\"\n",
    "        エフェクトパラメータを設定\n",
    "        \"\"\"\n",
    "        if effect_name in self.effects:\n",
    "            self.effects[effect_name] = value\n",
    "            \n",
    "            # フィルタパラメータが変更された場合は再計算\n",
    "            if effect_name in ['lowpass_freq', 'highpass_freq']:\n",
    "                self.update_filters()\n",
    "\n",
    "# リアルタイム処理のデモ\n",
    "processor = RealTimeAudioProcessor()\n",
    "simulator = AudioStreamSimulator()\n",
    "\n",
    "# 異なるエフェクト設定でテスト\n",
    "effect_configs = [\n",
    "    {'name': 'Original', 'effects': []},\n",
    "    {'name': 'Gain Only', 'effects': ['gain']},\n",
    "    {'name': 'Filtered', 'effects': ['gain', 'filters']},\n",
    "    {'name': 'With Echo', 'effects': ['gain', 'filters', 'echo']},\n",
    "    {'name': 'Full Processing', 'effects': ['gain', 'filters', 'echo', 'compressor']}\n",
    "]\n",
    "\n",
    "# パラメータ調整\n",
    "processor.set_effect_parameter('gain', 1.5)\n",
    "processor.set_effect_parameter('lowpass_freq', 300)\n",
    "processor.set_effect_parameter('echo_decay', 0.4)\n",
    "\n",
    "# 各設定でのテスト\n",
    "fig, axes = plt.subplots(len(effect_configs), 1, figsize=(15, 2*len(effect_configs)))\n",
    "\n",
    "# テスト用音響データ生成\n",
    "test_chunk, test_time = simulator.generate_audio_chunk()\n",
    "\n",
    "for i, config in enumerate(effect_configs):\n",
    "    if config['name'] == 'Original':\n",
    "        processed_chunk = test_chunk\n",
    "    else:\n",
    "        processed_chunk = processor.process_chunk(test_chunk, config['effects'])\n",
    "    \n",
    "    axes[i].plot(test_time, processed_chunk)\n",
    "    axes[i].set_title(f'{config[\"name\"]} (Effects: {config[\"effects\"]})')\n",
    "    axes[i].set_ylabel('Amplitude')\n",
    "    axes[i].grid(True)\n",
    "    \n",
    "    # RMS計算\n",
    "    rms = np.sqrt(np.mean(processed_chunk**2))\n",
    "    axes[i].text(0.02, 0.95, f'RMS: {rms:.3f}', transform=axes[i].transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "axes[-1].set_xlabel('Time (s)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# エフェクトパラメータの表示\n",
    "print(\"Current Effect Parameters:\")\n",
    "for param, value in processor.effects.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 統合リアルタイムシステムのデモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedAudioSystem:\n",
    "    \"\"\"\n",
    "    統合音響処理システム\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_rate=1000, chunk_size=256):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        # 各コンポーネントの初期化\n",
    "        self.simulator = AudioStreamSimulator(sample_rate, chunk_size)\n",
    "        self.monitor = RealTimeWaveformMonitor(buffer_size=1000, sample_rate=sample_rate)\n",
    "        self.analyzer = RealTimeSpectrumAnalyzer(fft_size=512, sample_rate=sample_rate)\n",
    "        self.detector = AudioEventDetector(sample_rate=sample_rate)\n",
    "        self.processor = RealTimeAudioProcessor(sample_rate=sample_rate)\n",
    "        \n",
    "        # システム状態\n",
    "        self.processing_stats = {\n",
    "            'chunks_processed': 0,\n",
    "            'total_events': 0,\n",
    "            'processing_time': 0,\n",
    "            'avg_rms': 0\n",
    "        }\n",
    "    \n",
    "    def process_single_frame(self, enable_effects=True):\n",
    "        \"\"\"\n",
    "        単一フレームを処理\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. 音響データ取得\n",
    "        chunk, time_chunk = self.simulator.generate_audio_chunk()\n",
    "        \n",
    "        # 2. エフェクト処理\n",
    "        if enable_effects:\n",
    "            processed_chunk = self.processor.process_chunk(chunk)\n",
    "        else:\n",
    "            processed_chunk = chunk\n",
    "        \n",
    "        # 3. モニタリング\n",
    "        self.monitor.update_buffer(processed_chunk, time_chunk)\n",
    "        \n",
    "        # 4. スペクトラム解析\n",
    "        spectrum_result = self.analyzer.analyze_chunk(processed_chunk)\n",
    "        \n",
    "        # 5. イベント検出\n",
    "        events = self.detector.detect_events(processed_chunk, spectrum_result)\n",
    "        \n",
    "        # 6. 統計更新\n",
    "        processing_time = time.time() - start_time\n",
    "        self.processing_stats['chunks_processed'] += 1\n",
    "        self.processing_stats['total_events'] += len(events)\n",
    "        self.processing_stats['processing_time'] += processing_time\n",
    "        self.processing_stats['avg_rms'] = (\n",
    "            (self.processing_stats['avg_rms'] * (self.processing_stats['chunks_processed'] - 1) + \n",
    "             spectrum_result['rms']) / self.processing_stats['chunks_processed']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'original_chunk': chunk,\n",
    "            'processed_chunk': processed_chunk,\n",
    "            'time_chunk': time_chunk,\n",
    "            'spectrum_result': spectrum_result,\n",
    "            'events': events,\n",
    "            'processing_time': processing_time\n",
    "        }\n",
    "    \n",
    "    def run_demo(self, num_frames=50, enable_effects=True):\n",
    "        \"\"\"\n",
    "        デモンストレーション実行\n",
    "        \"\"\"\n",
    "        print(f\"Running integrated audio system demo for {num_frames} frames...\")\n",
    "        print(f\"Effects enabled: {enable_effects}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for frame in range(num_frames):\n",
    "            result = self.process_single_frame(enable_effects)\n",
    "            results.append(result)\n",
    "            \n",
    "            # 重要なイベントを表示\n",
    "            for event in result['events']:\n",
    "                if event['type'] in ['onset', 'high_energy', 'spectral_change']:\n",
    "                    print(f\"[Frame {frame:2d}] {event['description']}\")\n",
    "            \n",
    "            # 進行状況表示\n",
    "            if (frame + 1) % 10 == 0:\n",
    "                print(f\"Processed {frame + 1}/{num_frames} frames...\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_system_report(self):\n",
    "        \"\"\"\n",
    "        システムレポートを生成\n",
    "        \"\"\"\n",
    "        stats = self.processing_stats\n",
    "        if stats['chunks_processed'] == 0:\n",
    "            return \"No data processed yet.\"\n",
    "        \n",
    "        avg_processing_time = stats['processing_time'] / stats['chunks_processed']\n",
    "        total_duration = stats['chunks_processed'] * self.chunk_size / self.sample_rate\n",
    "        event_rate = stats['total_events'] / total_duration if total_duration > 0 else 0\n",
    "        \n",
    "        report = f\"\"\"\n",
    "System Performance Report:\n",
    "  Total chunks processed: {stats['chunks_processed']}\n",
    "  Total processing time: {stats['processing_time']:.3f} seconds\n",
    "  Average processing time per chunk: {avg_processing_time*1000:.2f} ms\n",
    "  Audio duration processed: {total_duration:.2f} seconds\n",
    "  Real-time performance: {(total_duration/stats['processing_time']):.1f}x\n",
    "  \n",
    "Audio Analysis:\n",
    "  Total events detected: {stats['total_events']}\n",
    "  Event detection rate: {event_rate:.2f} events/second\n",
    "  Average RMS level: {stats['avg_rms']:.4f}\n",
    "  \n",
    "System Status: {'✓ Real-time capable' if avg_processing_time < (self.chunk_size/self.sample_rate) else '⚠ Not real-time capable'}\n",
    "\"\"\"\n",
    "        return report\n",
    "\n",
    "# 統合システムのデモ\n",
    "system = IntegratedAudioSystem()\n",
    "\n",
    "# システム設定\n",
    "system.processor.set_effect_parameter('gain', 1.2)\n",
    "system.processor.set_effect_parameter('lowpass_freq', 400)\n",
    "system.processor.set_effect_parameter('echo_decay', 0.3)\n",
    "\n",
    "# デモ実行\n",
    "demo_results = system.run_demo(num_frames=30, enable_effects=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(system.get_system_report())\n",
    "\n",
    "# 結果の可視化\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# 波形モニタリング\n",
    "time_data, audio_data = system.monitor.get_display_data()\n",
    "axes[0].plot(time_data, audio_data)\n",
    "axes[0].set_title('Real-time Waveform Monitor (Last 1000 samples)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# スペクトログラム\n",
    "spectrogram = system.analyzer.get_spectrogram()\n",
    "if spectrogram.shape[1] > 1:\n",
    "    im = axes[1].imshow(spectrogram, aspect='auto', origin='lower', \n",
    "                       extent=[0, spectrogram.shape[1], 0, system.analyzer.freqs[-1]], cmap='hot')\n",
    "    axes[1].set_title('Real-time Spectrogram')\n",
    "    axes[1].set_ylabel('Frequency (Hz)')\n",
    "    plt.colorbar(im, ax=axes[1], label='Magnitude')\n",
    "\n",
    "# 特徴量履歴\n",
    "features = system.analyzer.get_feature_history()\n",
    "if len(features['spectral_centroid']) > 0:\n",
    "    frame_indices = np.arange(len(features['spectral_centroid']))\n",
    "    axes[2].plot(frame_indices, features['spectral_centroid'], label='Spectral Centroid')\n",
    "    axes[2].plot(frame_indices, features['rms'] * 1000, label='RMS x1000')  # スケール調整\n",
    "    axes[2].set_title('Audio Features Over Time')\n",
    "    axes[2].set_xlabel('Frame Index')\n",
    "    axes[2].set_ylabel('Value')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# イベント検出サマリー\n",
    "event_summary = system.detector.get_event_summary()\n",
    "print(\"\\nEvent Detection Summary:\")\n",
    "for event_type, count in event_summary.items():\n",
    "    print(f\"  {event_type}: {count} occurrences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}